### 1. INTRODUCTION

---

- THREE key capabilities:
  - publish and subscribe to streams of records, similar to a mq
  - store streams of records in a fault-tolerant durable(容错持久的) way
  - process streams of records as they occur.

- using it to build:
  - real-time streaming data pipelines that reliably get data between systems or applications
  - real-time streaming applications that transform or react to the streams of data.

- a few concepts:
  - run as a cluster on servers that can span multiple datacenters.
  - stores streams of records in categories called topics.
  - each record consists of (1)a key, (2)a value and (3)a timestamp.

- four core APIs:
  - Producer API
  - Consumer API
  - Streams API
  - Connector API

- Partitioned log: the Kafka cluster maintains a partitioned log
  - EACH partition is an ordered, immutable sequence of records
  - offset: the records in the partitions are each assigned a sequential ID(offset)

- Kafka's performance is effectively constant
  - storing data for a long time is not a problem.

- PERSIST: (logs/consumer perspective)
  - (logs)persist all published records:
    - The Kafka cluster durably persists all published records-whether or not they have been consumed-using a configurable retention period.
  - (consumer)The ONLY metadata retained on a per-consumer basis:
    - is the OFFSET of that consumer in the log. Controlled by the consumer
      - (1) a consumer can reset to an older offset
      - (2) OR skip to the most recent record

- total order (within a partition):
  - Kafka only provides a total order over records within a partition. 
  - NOT between diff partitions in a topic.

- if you require a total order (within a topic):
  - only one partition -> only one consumer process per consumer group

- messaging:
  - (traditionally)two models: queuing & publish-subscribe
    - queuing: (strength+) divide up the processing of data over multiple consumer instances - scale processing
    - publish-subscribe: allows you broadcast data to multiple processes - multiple processes
  - (Kafka) it allows you to broad messages to multiple consumer group.
    - consumer group concept: it generalizes these two (queuing&publish-subscribe) concepts.
    - as with a queue, group can divide up processing over the members(of the consumer group)
    - as with publish-subscribe, it broadcast messages to multiple consumer groups.

- Advantage of Kafka:
  - every topic can scale processing and is also multi-subscriber. -no need to choose one or the other

- Kafka can provide BOTH ordering guarantees & load balancing over a pool of consumer processes.
  - EACH partition is consumed by exactly one consumer in the group
  - NOTE: there CANNOT be more consumer instances in a consumer group  than partitions

- Kafka is a very good storage system.(it allows producers to wait ack)
  - Data is wirtten to disk
  - replicated for fault-tolerance

- Kafka is a special distributed filesystem dedicated to high-performance, low-latency commit log storage, replication, and propagation.
  - it takes storage seriously and allows clients to control their read position.

- Stream Processing:
  - a stream processor is anything that takes continual streams of data from input topics, performs some processing on this input, and produces continual streams of data to output topics
  - to (1) compute aggregations off OR (2) join streams together. 
  - e.g.: handling out-of-order data, reprocessing input as code changes, performing stateful computations, etc.
  - Streams API builds on the core primitives Kafka.
 
  
- Kafka plays a role as streaming platform - combination of messaging, storage, and stream processing. 
  - (streaming applications)it allows storing and processing historical data from the past, like HDFS.
  - (streaming data pipelines)it allows processing future messages that will arrive after you subscribe.
 
---

##### 1.2. Use Cases

- cases:
  - Messaging(good solution for large scale msg processing applications): works as a replacement for a traditional message broker
    - To decouple processing from data producers(traditional msg broker)
    - To buffer unprocessed messages(traditional msg broker)
    - better throughput
    - built-in partition
    - repication
    - fault-tolerance
  - website activity tracking: a set of real-time publish-subscribe feeds.
    - page views, searches, etc.
    - one topic per activity type.
  - metrics: aggregating statistics from distributed applications to produce centralized feeds of operational data.
  - log aggregation: a log aggregation solution.
    - to collect physical log files off servers and put them in a central place.
    - compared with log-centric sys Scribe or Flume
      - to provide equally good performances, stronger durability guarantees and lower end-to-end latency
  - stream processing: process data in pipelines consisting of multiple stages.
    - consume raw input data
    - aggregated, enriched or transformed into new topics
      - e.g.: crawl article -> publish to a topic -> normalize/deduplicate content -> publish cleansed article to a new topic -> recommend
      - as of 0.10.0.0, the light-weight library Kafka Streamscan perform such data processing.
  - event sourcing: state changes are logged as a time-ordered sequence of records
  - commit log: similar to Apache BookKeeper
    - external commit-log for a distributed system.

---

##### 1.3 Quick Start

- 1.download: 

  ```  
  tar -xzf kafka_2.11-1.1.0.tgz
  cd kafka_2.11-1.1.0
  ```
- 2.start the server:

  ```
  # quick-and-dirty single-node ZooKeeper instance
  bin/zookeeper-server-start.sh config/zookeeper.properties
  ```

- 3.create a topic:
 
  ```
  bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
  bin/kafka-topics.sh --list --zookeeper localhost:2181
  ```

- 4.send msg:
  
  ```
  bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
  This is a message
  This is another message
  ```

- 5.start a consumer:
  
  ```
  bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning

  ```
- 6.Setting up a multi-broker cluster

  ```
  cp config/server.properties config/server-1.properties
  cp config/server.properties config/server-2.properties

  # config/server-1.properties:
  #   broker.id=1
  #   listeners=PLAINTEXT://:9093
  #   log.dir=/tmp/kafka-logs-1
 
  # config/server-2.properties:
  #   broker.id=2
  #   listeners=PLAINTEXT://:9094
  #   log.dir=/tmp/kafka-logs-2

  bin/kafka-server-start.sh config/server-1.properties &
  bin/kafka-server-start.sh config/server-2.properties &

  #create a topic with a replication factor of three
  bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic
  #check topic status:
  bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
  #test
  bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
  bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic
  #test out fault-tolerance
  ps aux | grep server-1.properties
  kill -9 <PID>
  bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
  ```

- Use Kafka Connect to import/export data:
  
  ```
  echo -e "foo\nbar" > test.txt
  bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
  more test.sink.txt
  bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning
  #{"schema":{"type":"string","optional":false},"payload":"foo"}
  #{"schema":{"type":"string","optional":false},"payload":"bar"}

  ```

- 8.Use Kafka Streams to process data:
  - Kafka Streams is a client library for building mission-critical real-time applications and microservices

---

### 2. APIs

- 4 APIs:
  - Producer API: allows applications to send streams of data to topics
  - Consumer API: allows to read streams of data from topics 
  - Streams API: allows transforming streams of data from input topics to output topics
  - Connect API: alows implementing connectors -> continually pulll from from source sys/app into kafka OR push from Kafka to sink sys/app
  - AdminClient API: allows managing/inspecting topics, brokers, and other Kafka objects.

  ```
  # enable producer/consumer/AdminClient API:
  <dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>1.1.0</version>
  </dependency>

  # streams API:
  <dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams</artifactId>
    <version>1.1.0</version>
  </dependency>
  ```
 
